rename(predicted_label = .pred_class, actual_label = label) %>%
table()
rownames(bag.cm) <- c("Good", "Bad")
colnames(bag.cm) <- c("Good", "Bad")
bag.cm
# set model, no. of predictor to be selected through cross-validation
cd.rf <- rand_forest(trees=100, mtry=tune()) %>%
set_engine("randomForest") %>%
set_mode("classification")
# set random forest workflow
cd.workflow.rf <- workflow() %>%
add_recipe(cd.recipe) %>%
add_model(cd.rf)
# tune random forest
set.seed(500)
cd.fit.rf <- tune_grid(cd.workflow.rf, resamples=card.fold, metrics=metric_set(roc_auc))
cd.rf.tune.param <- cd.fit.rf %>% select_best()
cd.workflow.rf.final <- finalize_workflow(cd.workflow.rf, cd.rf.tune.param)
# fit final model to train data and evaluate on test data
cd.rf.eval <- cd.workflow.rf.final %>%
last_fit(split=card.split)
# AUC value for random forest
rf.metrics <- cd.rf.eval %>%
collect_metrics()
rf.metrics
# plot ROC
rf.roc <- cd.rf.eval %>%
collect_predictions() %>%
roc_curve(truth=label, .pred_0) %>%
autoplot()
rf.roc
# important variables
rf.vip <- cd.workflow.rf.final %>%
fit(data=card.train) %>%
pull_workflow_fit() %>%
vip()
rf.vip
# confusion matrix
rf.cm <- cd.rf.eval %>%
collect_predictions() %>%
dplyr::select(.pred_class, label) %>%
rename(predicted_label = .pred_class, actual_label = label) %>%
table()
rownames(rf.cm) <- c("Good", "Bad")
colnames(rf.cm) <- c("Good", "Bad")
rf.cm
# combine accuracy values for three methods
accuracy_sum <- list(tree.metrics[1,], bag.metrics[1,], rf.metrics[1,]) %>%
map_dfr(bind_rows) %>%
dplyr::select(accuracy=.estimate)
# combine AUC values for three methods
auc_sum <- list(tree.metrics[2,], bag.metrics[2,], rf.metrics[2,]) %>%
map_dfr(bind_rows) %>%
dplyr::select(AUC=.estimate)
# display performance metrics of all three methods
all_tree_metrics <- cbind(method=c("Tree", "Bagging", "Random Forest"), auc_sum, accuracy_sum)
all_tree_metrics
# combine AUC values for all models
model_auc_sum <- list(tree.metrics[2,], bag.metrics[2,], rf.metrics[2,],svmrbf.metrics[2,]) %>%
map_dfr(bind_rows) %>%
dplyr::select(AUC=.estimate)
#Regularization can be used to train models that generalize better on unseen data, by preventing the algorithm from overfitting the training data set.
x <- model.matrix(label~., card.train)[,-16]
y <- card.train$label
glmnet(x, y, family = "binomial", alpha = 1, lambda = NULL)
cd.bag.eval %>% collect_predictions()
bag.predict <- cd.bag.eval %>% collect_predictions()
## prepare an object for ROC curve
bag.prediction <- prediction(bag.predict$.pred_1, card.test$label)
bag.roc.obj <- performance(bag.prediction, measure="tpr", x.measure="fpr")
## plot ROC curve
plot(bag.roc.obj, main="Decision bag: ROC curve on testing data")
## get the probabilities from rf model
rf.predict <- cd.rf.eval %>% collect_predictions()
## prepare an object for ROC curve
rf.prediction <- prediction(rf.predict$.pred_1, card.test$label)
rf.roc.obj <- performance(rf.prediction, measure="tpr", x.measure="fpr")
## plot ROC curve
plot(rf.roc.obj, main="Decision rf: ROC curve on testing data")
rf.roc
# check performance
## fit final model on training data and evaluate performance on test data
cd.tree.eval <- cd.workflow.tree.final %>%
last_fit(card.split)
## AUC value for decision tree
tree.metrics <- cd.tree.eval %>%
collect_metrics()
tree.metrics
# plot ROC
## get the probabilities from tree model
tree.predict <- cd.tree.eval %>% collect_predictions()
## prepare an object for ROC curve
tree.prediction <- prediction(tree.predict$.pred_1, card.test$label)
tree.roc.obj <- performance(tree.prediction, measure="tpr", x.measure="fpr")
## plot ROC curve
plot(tree.roc.obj, main="Decision tree: ROC curve on testing data")
plot(cd.workflow.tree.final)
autoplot(cd.workflow.tree.final)
View(cd.tree.eval)
tree.fit <-cd.workflow.tree.final %>%
pull_workflow_fit()
rpart.plot(tree.fit$fit)
library("rpart.plot")
rpart.plot(tree.fit$fit)
tree.fit <- tree.eval %>%
pull_workflow_fit()
tree.fit <- cd.tree.eval %>%
pull_workflow_fit()
tree.fit <- cd.workflow.tree %>%
fit(card.train) %>%
pull_workflow_fit()
cd.tree.tune.param
tree.fit <- cd.workflow.tree %>%
fit(card.train) %>%
pull_workflow_fit()
cd.workflow.tree %>%
fit(card.train)
tree.fit <- cd.workflow.tree.final %>%
fit(card.train) %>%
pull_workflow_fit()
rpart.plot(tree.fit$fit)
rpart.plot(tree.fit$fit,model=TRUE)
rpart.plot(tree.fit$fit, ropundint=FALSE)
rpart.plot(tree.fit$fit, roundint=FALSE)
cd.tree.tune.param
library(tidymodels)
library(baguette)
library(Information)
library(ROCR)
library(caret)
library(vip)
library(rpart.plot)
library(gt)
library(visdat)
library(GGally)
library(car)
library(cutpointr)
library(bestglm)
library(glmnet)
library(ggplot2)
library(reshape2)
library(ggrepel)
library(MASS)
tree.fit <- cd.workflow.tree.final %>%
fit(card.train) %>%
pull_workflow_fit()
rpart.plot(tree.fit$fit, roundint=FALSE)
beta_f
rpart.plot(tree.fit$fit, roundint=FALSE)
## plot ROC curve
plot(tree.roc.obj, main="Decision tree: ROC curve on testing data")
# check variables that are important in the tree: (reference**)
tree.vip <- cd.workflow.tree.final %>%
fit(data=card.train) %>% # fit model on training data
pull_workflow_fit() %>%
vip()
tree.vip
# confusion matrix
tree.cm <- cd.tree.eval %>%
collect_predictions() %>%
dplyr::select(.pred_class, label) %>%
rename(predicted = .pred_class, actual = label) %>%
table()
rownames(tree.cm) <- c("Good", "Bad")
colnames(tree.cm) <- c("Good", "Bad")
tree.cm
# check variables that are important in the tree: (reference**)
tree.vip <- cd.workflow.tree.final %>%
fit(data=card.train) %>% # fit model on training data
pull_workflow_fit() %>%
vip()
tree.vip
glm2.auc
tree.metrics
tree.cm
# set bag model, leave cost_complexity for tuning
cd.bag <- bag_tree(tree_depth = 11, cost_complexity = tune("C")) %>%
set_engine("rpart", times=5) %>%
set_mode("classification")
# set workflow
cd.workflow.bag <- workflow() %>%
add_recipe(cd.recipe) %>%
add_model(cd.bag)
# model tuning
set.seed(280)
# grid is set after several trials
cd.fit.bag <- tune_grid(cd.workflow.bag, grid=data.frame(C=2^(-13:-8)), resamples=card.fold, metrics=metric_set(roc_auc))
## select the best combination
cd.bag.tune.param <- cd.fit.bag %>% select_best()
## finalise workflow bag
cd.workflow.bag.final <- finalize_workflow(cd.workflow.bag, cd.bag.tune.param)
cd.bag.tune.param
# check variables that are important
bag.vip.bagger <- cd.workflow.bag.final %>%
fit(data=card.train) %>% # fit model on training data
pull_workflow_fit()
# extract variable importance from _bagger class
bag.vip <- bag.vip.bagger$fit$imp %>%
# user reorder to ensure the descending order of importance
ggplot(aes(y=reorder(term, value), x=value)) + geom_col() +
ylab("") + xlab("Importance")
bag.vip
# check performance
## fit final model on training data and evaluate performance on test data
cd.bag.eval <- cd.workflow.bag.final %>%
last_fit(card.split)
## AUC value for decision bag
bag.metrics <- cd.bag.eval %>%
collect_metrics()
bag.metrics
# plot ROC
## get the probabilities from bag model
bag.predict <- cd.bag.eval %>% collect_predictions()
## prepare an object for ROC curve
bag.prediction <- prediction(bag.predict$.pred_1, card.test$label)
bag.roc.obj <- performance(bag.prediction, measure="tpr", x.measure="fpr")
## plot ROC curve
plot(bag.roc.obj, main="Bagging: ROC curve on testing data")
# confusion matrix
bag.cm <- cd.bag.eval %>%
collect_predictions() %>%
dplyr::select(.pred_class, label) %>%
rename(predicted_label = .pred_class, actual_label = label) %>%
table()
rownames(bag.cm) <- c("Good", "Bad")
colnames(bag.cm) <- c("Good", "Bad")
bag.cm
cd.rf.tune.param
# important variables
rf.vip <- cd.workflow.rf.final %>%
fit(data=card.train) %>%
pull_workflow_fit() %>%
vip()
rf.vip
# fit final model to train data and evaluate on test data
cd.rf.eval <- cd.workflow.rf.final %>%
last_fit(split=card.split)
# AUC value for random forest
rf.metrics <- cd.rf.eval %>%
collect_metrics()
rf.metrics
# plot ROC
## get the probabilities from rf model
rf.predict <- cd.rf.eval %>% collect_predictions()
## prepare an object for ROC curve
rf.prediction <- prediction(rf.predict$.pred_1, card.test$label)
rf.roc.obj <- performance(rf.prediction, measure="tpr", x.measure="fpr")
## plot ROC curve
plot(rf.roc.obj, main="Random forest: ROC curve on testing data")
# confusion matrix
rf.cm <- cd.rf.eval %>%
collect_predictions() %>%
dplyr::select(.pred_class, label) %>%
rename(predicted_label = .pred_class, actual_label = label) %>%
table()
rownames(rf.cm) <- c("Good", "Bad")
colnames(rf.cm) <- c("Good", "Bad")
rf.cm
# combine accuracy values for three methods
accuracy_sum <- list(tree.metrics[1,], bag.metrics[1,], rf.metrics[1,]) %>%
map_dfr(bind_rows) %>%
dplyr::select(accuracy=.estimate)
# combine AUC values for three methods
auc_sum <- list(tree.metrics[2,], bag.metrics[2,], rf.metrics[2,]) %>%
map_dfr(bind_rows) %>%
dplyr::select(AUC=.estimate)
# display performance metrics of all three methods
all_tree_metrics <- cbind(method=c("Tree", "Bagging", "Random Forest"), auc_sum, accuracy_sum)
all_tree_metrics
# combine AUC values for three methods
auc_sum <- list(tree.metrics[2,], bag.metrics[2,], rf.metrics[2,]) %>%
map_dfr(bind_rows) %>%
dplyr::select(AUC=.estimate)
# display performance metrics of all three methods
all_tree_metrics <- cbind(method=c("Tree", "Bagging", "Random Forest"), auc_sum)
all_tree_metrics
glm2.0 <- glm(Y ~ X, family=binomial)
summary(glm2.0)
library(tidymodels)
library(baguette)
library(Information)
library(ROCR)
library(caret)
library(vip)
library(rpart.plot)
library(gt)
library(visdat)
library(GGally)
library(car)
library(cutpointr)
library(bestglm)
library(glmnet)
library(ggplot2)
library(reshape2)
library(ggrepel)
library(MASS)
credit.dat <- read.csv("credit_record.csv")
setwd("~/Documents/好好学习！/R/ST310/Machine-Learning-Final-Project")
library(tidymodels)
library(baguette)
library(Information)
library(ROCR)
library(caret)
library(vip)
library(rpart.plot)
library(gt)
library(visdat)
library(GGally)
library(car)
library(cutpointr)
library(bestglm)
library(glmnet)
library(ggplot2)
library(reshape2)
library(ggrepel)
library(MASS)
credit.dat <- read.csv("credit_record.csv")
credit.dat  %>%
slice_head(n = 4) %>% #select the top 4 rows
gt() # print output using gt
application.dat <- read.csv("application_record.csv")
application.dat  %>%
slice_head(n = 4) %>%  #select the top 4 rows
gt() # print output using gt
glimpse(application.dat)
glimpse(credit.dat)
# remove those account with insufficient history (less than 3 months)
credit.1 <- credit.dat %>%
group_by(ID) %>%
mutate(min_month=min(MONTHS_BALANCE)) %>%
filter(min_month < -2)
# label 1 for those have ever been three or more months overdue
positive_label <- credit.dat %>%
filter(STATUS == 3 | STATUS == 4 |STATUS == 5) %>%
distinct(ID) %>%
mutate(label = 1)
# Join the records with the same ID
credit.label <- full_join(credit.1, positive_label, by="ID")
# label 0 for those who are not 1
negative_label <- credit.label %>%
filter(is.na(label)) %>%
mutate(label = 0) %>%
dplyr::select(ID, label)
# create a data frame with ID and label
id_label <- rbind(positive_label, negative_label) %>%
distinct(ID, label)
# merge both data frame to include predictor variables and outcome variable by ID (removing accounts with no credit record)
card.dat <- inner_join(application.dat, id_label, by="ID")
# columns with "characters" into
cols <- c("CODE_GENDER", "FLAG_OWN_CAR", "FLAG_OWN_REALTY", "NAME_INCOME_TYPE",
"NAME_EDUCATION_TYPE", "NAME_FAMILY_STATUS", "NAME_HOUSING_TYPE", "FLAG_MOBIL",
"FLAG_WORK_PHONE", "FLAG_PHONE", "FLAG_EMAIL", "OCCUPATION_TYPE", "label")
# coerce these variables to factors
card.dat[cols] <- lapply(card.dat[cols], factor)
summary(card.dat)
summary(card.dat$DAYS_BIRTH)
summary(card.dat$DAYS_EMPLOYED)
#Negative numbers due to counting days backward
#Revert to counting days forward
#converting Birthday and Employed day to years
card.dat$age <- -card.dat$DAYS_BIRTH%/%365
#365243 in DAYS_EMPLOYED refers to unemployed by Data Dictionary
card.dat$years_employed <- ifelse(card.dat$DAYS_EMPLOYED != 365243,round(-card.dat$DAYS_EMPLOYED/365, digits=2), 0)
# correlation between CNT_CHILDREN and CNT_FAM_MEMBERS
ggplot(data=card.dat, aes(x=CNT_CHILDREN, y=CNT_FAM_MEMBERS)) +
stat_sum(aes(size = factor(..n..)), geom = "point") + scale_size_discrete(name = "Count")
card.dat <- card.dat %>% dplyr::select(-CNT_CHILDREN)
# calculate the percentage of "good/bad" in each level
famcnt.tab <- table(card.dat$CNT_FAM_MEMBERS, card.dat$label)
famcnt.pro.tab <- prop.table(famcnt.tab)
famcnt.df.b <- data.frame(famcnt.pro.tab)[10:20,] # data for "bad" account
# plot
famcnt.p <- ggplot(famcnt.df.b) + geom_col(aes(x=Var1, y=100*Freq)) +
xlab("Family Count") + ylab("Perc. of Bad Accounts, %")
famcnt.p
card.dat$CNT_FAM_MEMBERS[card.dat$CNT_FAM_MEMBERS >= 5] <- "5+"
card.dat$CNT_FAM_MEMBERS[card.dat$CNT_FAM_MEMBERS == 4] <- "4"
card.dat$CNT_FAM_MEMBERS[card.dat$CNT_FAM_MEMBERS == 3] <- "3"
card.dat$CNT_FAM_MEMBERS[card.dat$CNT_FAM_MEMBERS == 2] <- "2"
card.dat$CNT_FAM_MEMBERS[card.dat$CNT_FAM_MEMBERS == 1] <- "1"
card.dat$CNT_FAM_MEMBERS <- as.factor(card.dat$CNT_FAM_MEMBERS)
table(card.dat$CNT_FAM_MEMBERS, card.dat$label)
plot(ecdf(card.dat$AMT_INCOME_TOTAL),main="Empirical CDF",xlab="INCOME")
histogram(card.dat$AMT_INCOME_TOTAL,main="PDF",xlab="INCOME")
card.dat$AMT_INCOME_TOTAL <- log(card.dat$AMT_INCOME_TOTAL)
summary(card.dat$FLAG_MOBIL)
# Removing FLAG_MOBIL because it has only 1 unique value"
card.dat <- card.dat %>% dplyr::select(-FLAG_MOBIL)
table(card.dat$NAME_INCOME_TYPE, card.dat$label)
card.dat <- subset(card.dat, card.dat$NAME_INCOME_TYPE!="Student")
card.dat$NAME_INCOME_TYPE <- droplevels(card.dat$NAME_INCOME_TYPE)
table(card.dat$NAME_EDUCATION_TYPE, card.dat$label)
levels(card.dat$NAME_EDUCATION_TYPE) <- c("Higher education","Higher education",
"Incomplete higher", "Lower secondary",
"Secondary / secondary special")
table(card.dat$NAME_FAMILY_STATUS, card.dat$label)
table(card.dat$NAME_HOUSING_TYPE, card.dat$label)
# Distribution of years of employment for people who do not specify occupation type
card.dat %>%
filter(OCCUPATION_TYPE == "") %>%
group_by(group = cut(years_employed,
breaks = c(-0.001, 0, 10, 20, 30, round(max(years_employed))),
include.lowest = FALSE)) %>%
count() %>%
rename(Years_Employed=group, Number_of_People=n)
card.dat <- card.dat %>%
mutate(occupation_type = as.factor(case_when(
# create level unemployed and unknown for occupation type
OCCUPATION_TYPE=="" & years_employed==0 ~ "Unemployed",
OCCUPATION_TYPE=="" & years_employed>0 ~ "Unknown",
# recode other levels to "HighSkill" and "LowSkill"
OCCUPATION_TYPE %in% c("Accountants", "Core Staff", "High skill tech staff",
"HR staff", "IT staff", "Managers", "Medicine staff") ~ "HighSkill",
TRUE ~ "LowSkill"
)))
# remove the ID and original variables for age, years_employed and occupation type
card.dat.new <- card.dat %>%
dplyr::select(-c(ID, DAYS_BIRTH, DAYS_EMPLOYED, OCCUPATION_TYPE))
# move label to be the last column
card.dat.new <- card.dat.new[c(1:12, 14, 15, 16, 13)]
# Count of Bad account
sum(as.numeric(card.dat$label)-1)
# Proportion of Bad account
badprop=sum(as.numeric(card.dat$label)-1)/length(card.dat$label)
badprop
# Proportion of Good account
1-badprop
# change response variable to be numeric in order to use create_infotables function
card.dat.new1 <- card.dat.new
card.dat.new1$label <- as.numeric(as.character(card.dat.new$label))
# calculate the information value for each variable
IV <- create_infotables(data=card.dat.new1, y="label", parallel=FALSE)
print(IV$Summary)
# calculate the percentage of "good/bad" in each level
fam.tab <- table(card.dat.new$NAME_FAMILY_STATUS, card.dat.new$label)
fam.pro.tab <- prop.table(fam.tab)
fam.df.b <- data.frame(fam.pro.tab)[6:10,] # data for "bad" accounts
# plot
fam.p <- ggplot(fam.df.b) +
geom_col(aes(x=Var1, y=100*Freq)) +
xlab("Family Status") +
ylab("Perc. of Bad Accounts, %")
fam.p
# split training and testing dataset
set.seed(100)
card.split <- initial_split(card.dat.new, prop=3/4, strata="label")
card.train <- training(card.split)
card.test <- testing(card.split)
set.seed(103)
card.fold <- vfold_cv(card.train, v=5, strata="label")
# Fit the model
full_model <- glm(label ~., data = card.train, family = binomial)
vif(full_model)
full_model <- glm(label ~.-NAME_INCOME_TYPE, data=card.train, family=binomial)
vif(full_model)
# since gender only has two levels, we recode it to be a numeric variable
# with values 1 (for Male) and 2 (for Female) for simplicity in coding
x_gender <- as.numeric(card.train$CODE_GENDER)
x_emp <- card.train$years_employed
X <- cbind(rep(1, length(x_gender)), x_gender, x_emp)
Y <- as.matrix(as.numeric(as.character(card.train$label)))
# logit function
g <- function(p) return(z = log(p / (1-p)))
# inverse of logit function (sigmoid function)
g_inv <- function(z) return(p = 1 / (1+exp(-z)))
# loss function [referecne **]
loss_fn <- function(beta, X, Y) {
m <- dim(X)[1] # no. of observations
p <- g_inv(X%*%beta)
J <- (t(-Y) %*% log(p) - t(1-Y) %*% log(1-p))/m
return(J)
}
# gradient of loss function
grd_descent <- function(X, Y, beta, step_size) {
m <- dim(X)[1]  # no. of observations
# set random values in order to enter the while loop
loss_t0 <- 100 ; loss_t1 <- 1
iter <- 0
while(abs(loss_t0 - loss_t1) > 0.000001) {
loss_t0 <- loss_fn(beta, X, Y) # calculate initial loss
p <- g_inv(X %*% beta) # probabilities at t0
grad <- (1/m) * t(X)%*%(p - Y) # gradient of the loss function
# update new estimates of beta (at t1)
beta <- beta - (0.999)^iter * grad / sqrt(sum(grad^2))
loss_t1 <- loss_fn(beta, X, Y) # calculate new loss
iter <- iter + 1
}
return(beta)
}
# initialise beta
beta <- as.matrix(c(0, 0, 0))
# get coefficient after gradient descent
beta_f <- grd_descent(X, Y, beta, 0.001)
beta_f
glm2.0 <- glm(Y ~ X, family=binomial)
summary(glm2.0)
set.seed(13)
beta_f <- grd_descent(X, Y, beta, 0.001)
beta_f
set.seed(40)
beta_f <- grd_descent(X, Y, beta, 0.001)
beta_f
set.seed(128)
beta_f <- grd_descent(X, Y, beta, 0.001)
beta_f
set.seed(228)
beta_f <- grd_descent(X, Y, beta, 0.001)
beta_f
beta_f <- grd_descent(X, Y, beta, 0.001)
beta_f
# years_employed and label
ggplot(data=card.dat.new, aes(y=years_employed, x=label)) + geom_boxplot()
